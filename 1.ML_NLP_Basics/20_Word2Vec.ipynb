{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c838c3",
   "metadata": {},
   "source": [
    "### Word2Vec - \n",
    "- CBOW (Continuous Bag of Words) - **FIND OUT MISSING WORD**\n",
    "- Skip-Gram - **FIND OUT PREDICT WORD**\n",
    "\n",
    "- **small data set = CBOW**\n",
    "- **large data set = skip-gram**\n",
    "\n",
    "- imporve:-\n",
    "    - incresing The traning data\n",
    "    - incresing the window size vector size also incresing\n",
    "\n",
    "#### CBOW (Continuous Bag of Words) :\n",
    "\n",
    "**\"The cat sits on the mat\"**\n",
    "- And the context window size = 2\n",
    "\n",
    "-To predict the word \"sits\", CBOW will use:\n",
    "\n",
    "**[\"The\", \"cat\", \"on\", \"the\"] ➝ Predict \"sits\"**\n",
    "- CBOW looks at the context (surrounding words) and tries to predict the target word (center word).\n",
    "\n",
    "🔄 How CBOW Works (Step-by-step)\n",
    "Let’s understand it deeply:\n",
    "\n",
    "##### 1. Input: Context Words\n",
    "- Suppose your context size = 2. For the sentence:\n",
    "\n",
    "**I love natural language processing**\n",
    "\n",
    "- If the center word is \"natural\", context words are:\n",
    "\n",
    "- [\"I\", \"love\", \"language\", \"processing\"]\n",
    "\n",
    "##### 2. One-Hot Encoding\n",
    "- Each context word is represented as a one-hot encoded vector (vector of 0s with a single 1 at the index of the word in the vocabulary).\n",
    "\n",
    "- Example:\n",
    "    - \"I\"       = [1, 0, 0, 0, ..., 0]\n",
    "    - \"love\"    = [0, 1, 0, 0, ..., 0]\n",
    "##### 3. Projection Layer (Embedding Matrix)\n",
    "- All one-hot vectors are multiplied by a shared embedding matrix (W) to convert them into dense word vectors.\n",
    "- Then average these vectors (or sum).\n",
    "\n",
    "##### 4. Hidden Layer\n",
    "- This average vector is passed through a hidden layer (just matrix multiplication – no activation function).\n",
    "\n",
    "##### 5. Output Layer (Softmax)\n",
    "- It outputs a probability distribution over the vocabulary using softmax. The word with the highest probability is the predicted center word.\n",
    "\n",
    "##### 6. Loss Function\n",
    "- Loss is calculated (usually cross-entropy loss) by comparing the predicted word with the actual target word.\n",
    "\n",
    "##### 7. Training\n",
    "- Weights of the embedding matrix are updated using backpropagation + gradient descent to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559254c",
   "metadata": {},
   "source": [
    "##### Adavantegges\n",
    "\n",
    "- sparse matrix ----------> dense matrix\n",
    "- semantic info capture  \n",
    "- vocablory size  ----------> fixe set of dimention (google dimention is [300 dimention])\n",
    "- out os vocablory (oov) is also solved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c441acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec , keyedvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b2239",
   "metadata": {},
   "source": [
    "###### Refrence link :- https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They are run around 1800 MB requried\n",
    "# genrated the vectore\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94816b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('happy', 'rahul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = wv['king']-wv['man']+wv['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9caa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar([vec])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
